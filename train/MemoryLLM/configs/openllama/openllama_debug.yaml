model:
  base_learning_rate: 4.6e-6
  target: MemoryLLM.memoryllm.models.memory.LlamaMemoryModelPL
  params:
    monitor: val/avg_acc
    num_blocks: 4
    num_tokens: 256
    update_memory_during_training: true
    lora_config:
      inference_mode: false
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ['q_proj', 'v_proj', 'k_proj', 'up_proj', 'down_proj', 'gate_proj']
    module_name: LlamaDropMemoryModel
    model_path: openlm-research/open_llama_3b_v2
    add_bos_embedding: true
    cat_memories: false
    cat_and_drop_memory: true
    shuffle_contexts: false
    detach_additional_memory: false
    cache_data_for_longer_context: true
    shrink_to_one_embedding: true
    occassionally_cat_memory_when_one_context_ratio: 0.5
    drop_memory_per_layer: true
    rope_scaling: 
      type: linear
    num_contexts_schedule:
      checkpoints: [1000, 2500, 5000, 10000, 20000, 30000]
      values: [1, 2, 3, 4, 5, 10, 20]
    # validation_dataset_names:
    #   - naturalqa
    #   - squad

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 1
    eval_batch_size: 1
    eval_max_length: 462
    num_workers: 16
    num_tokens: 256
    wrap: False
    use_worker_init_fn: true
    worker_init_fn: worker_init_fn_redpajama
    train:
      target: MemoryLLM.memoryllm.data.redpajama.RedPajamaDataset
      params:
        root: ./data/redpajama
        tokenizer: llama
        tokenizer_path: openlm-research/open_llama_3b_v2
        num_tokens: 256
        max_length: 512
        languages: ['en']
        snapshots: ["2022-40"]
        partition: head_middle
        # end_special_token: </s>

    # validation:
    #   - target: MemoryLLM.memoryllm.data.nq.NQDataset
    #     params:
    #       filename: ./data/nq/v1.0-simplified_nq-dev-all.jsonl
    #       num_unrelated_contexts: 5
    #       num: 100
    #   - target: MemoryLLM.memoryllm.data.squad.SQuADDataset
    #     params:
    #       filename: ./data/squad/dev-v2.0.json
    #       num_unrelated_contexts: 5
    #       num: 100

lightning:
  trainer:
    accelerator: gpu
    strategy: deepspeed_stage_2
    limit_train_batches: 10000
    precision: 16-mixed
    accumulate_grad_batches: 4