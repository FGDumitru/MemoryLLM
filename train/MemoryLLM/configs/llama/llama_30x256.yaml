model:
  base_learning_rate: 4.6e-6
  target: MemoryLLM.memoryllm.models.memory.LlamaMemoryModelPL
  params:
    monitor: val/avg_acc
    num_blocks: 30
    num_tokens: 256
    update_memory_during_training: true
    lora_config:
      inference_mode: false
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ['q_proj', 'v_proj', 'k_proj', 'up_proj', 'down_proj', 'gate_proj']
    module_name: LlamaDropMemoryModel
    model_path: meta-llama/Llama-2-7b-hf
    add_positional_embedding: true
    split_positional_embedding: true
    add_bos_embedding: true
    cat_memories: false
    cat_and_drop_memory: true
    shuffle_contexts: false
    detach_additional_memory: false
    cache_data_for_longer_context: true
    new_memory_embedding_fullset: true
    shrink_to_one_embedding: true
    occassionally_cat_memory_when_one_context_ratio: 0.5
    drop_memory_per_layer: true
    rope_scaling: 
      type: linear
    num_contexts_schedule:
      checkpoints: [10000, 15000, 20000, 30000, 50000, 60000]
      values: [1, 2, 3, 4, 5, 10, 20]
    validation_dataset_names:
      - naturalqa
      - squad

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 1
    eval_batch_size: 1
    eval_max_length: 462
    num_workers: 16
    num_tokens: 256
    wrap: False
    use_worker_init_fn: true
    worker_init_fn: worker_init_fn_redpajama
    train:
      target: MemoryLLM.memoryllm.data.redpajama.RedPajamaDataset
      params:
        root: ./data/redpajama
        tokenizer: llama
        tokenizer_path: meta-llama/Llama-2-7b-hf
        num_tokens: 256
        max_length: 512
        languages: ['en']
        snapshots: ["2023-14"]
        partition: head_middle
        # end_special_token: </s>

    validation:
      - target: MemoryLLM.memoryllm.data.nq.NQDataset
        params:
          filename: ./data/nq/v1.0-simplified_nq-dev-all.jsonl
          num_unrelated_contexts: 5
          num: 100
      - target: MemoryLLM.memoryllm.data.squad.SQuADDataset
        params:
          filename: ./data/squad/dev-v2.0.json
          num_unrelated_contexts: 5
          num: 100

lightning:
  trainer:
    accelerator: gpu
    strategy: deepspeed_stage_2
    limit_train_batches: 10000
    precision: 16-mixed
    accumulate_grad_batches: 4